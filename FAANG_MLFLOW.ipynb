{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mlflow in z:\\faang project\\env\\lib\\site-packages (2.20.3)\n",
      "Requirement already satisfied: mlflow-skinny==2.20.3 in z:\\faang project\\env\\lib\\site-packages (from mlflow) (2.20.3)\n",
      "Requirement already satisfied: Flask<4 in z:\\faang project\\env\\lib\\site-packages (from mlflow) (3.1.0)\n",
      "Requirement already satisfied: Jinja2<4,>=3.0 in z:\\faang project\\env\\lib\\site-packages (from mlflow) (3.1.5)\n",
      "Requirement already satisfied: alembic!=1.10.0,<2 in z:\\faang project\\env\\lib\\site-packages (from mlflow) (1.14.1)\n",
      "Requirement already satisfied: docker<8,>=4.0.0 in z:\\faang project\\env\\lib\\site-packages (from mlflow) (7.1.0)\n",
      "Requirement already satisfied: graphene<4 in z:\\faang project\\env\\lib\\site-packages (from mlflow) (3.4.3)\n",
      "Requirement already satisfied: markdown<4,>=3.3 in z:\\faang project\\env\\lib\\site-packages (from mlflow) (3.7)\n",
      "Requirement already satisfied: matplotlib<4 in z:\\faang project\\env\\lib\\site-packages (from mlflow) (3.10.1)\n",
      "Requirement already satisfied: numpy<3 in z:\\faang project\\env\\lib\\site-packages (from mlflow) (2.2.3)\n",
      "Requirement already satisfied: pandas<3 in z:\\faang project\\env\\lib\\site-packages (from mlflow) (2.2.3)\n",
      "Requirement already satisfied: pyarrow<20,>=4.0.0 in z:\\faang project\\env\\lib\\site-packages (from mlflow) (19.0.1)\n",
      "Requirement already satisfied: scikit-learn<2 in z:\\faang project\\env\\lib\\site-packages (from mlflow) (1.6.1)\n",
      "Requirement already satisfied: scipy<2 in z:\\faang project\\env\\lib\\site-packages (from mlflow) (1.15.2)\n",
      "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in z:\\faang project\\env\\lib\\site-packages (from mlflow) (2.0.38)\n",
      "Requirement already satisfied: waitress<4 in z:\\faang project\\env\\lib\\site-packages (from mlflow) (3.0.2)\n",
      "Requirement already satisfied: cachetools<6,>=5.0.0 in z:\\faang project\\env\\lib\\site-packages (from mlflow-skinny==2.20.3->mlflow) (5.5.2)\n",
      "Requirement already satisfied: click<9,>=7.0 in z:\\faang project\\env\\lib\\site-packages (from mlflow-skinny==2.20.3->mlflow) (8.1.8)\n",
      "Requirement already satisfied: cloudpickle<4 in z:\\faang project\\env\\lib\\site-packages (from mlflow-skinny==2.20.3->mlflow) (3.1.1)\n",
      "Requirement already satisfied: databricks-sdk<1,>=0.20.0 in z:\\faang project\\env\\lib\\site-packages (from mlflow-skinny==2.20.3->mlflow) (0.44.1)\n",
      "Requirement already satisfied: gitpython<4,>=3.1.9 in z:\\faang project\\env\\lib\\site-packages (from mlflow-skinny==2.20.3->mlflow) (3.1.44)\n",
      "Requirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in z:\\faang project\\env\\lib\\site-packages (from mlflow-skinny==2.20.3->mlflow) (8.5.0)\n",
      "Requirement already satisfied: opentelemetry-api<3,>=1.9.0 in z:\\faang project\\env\\lib\\site-packages (from mlflow-skinny==2.20.3->mlflow) (1.30.0)\n",
      "Requirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in z:\\faang project\\env\\lib\\site-packages (from mlflow-skinny==2.20.3->mlflow) (1.30.0)\n",
      "Requirement already satisfied: packaging<25 in z:\\faang project\\env\\lib\\site-packages (from mlflow-skinny==2.20.3->mlflow) (24.2)\n",
      "Requirement already satisfied: protobuf<6,>=3.12.0 in z:\\faang project\\env\\lib\\site-packages (from mlflow-skinny==2.20.3->mlflow) (5.29.3)\n",
      "Requirement already satisfied: pydantic<3,>=1.10.8 in z:\\faang project\\env\\lib\\site-packages (from mlflow-skinny==2.20.3->mlflow) (2.10.6)\n",
      "Requirement already satisfied: pyyaml<7,>=5.1 in z:\\faang project\\env\\lib\\site-packages (from mlflow-skinny==2.20.3->mlflow) (6.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.17.3 in z:\\faang project\\env\\lib\\site-packages (from mlflow-skinny==2.20.3->mlflow) (2.32.3)\n",
      "Requirement already satisfied: sqlparse<1,>=0.4.0 in z:\\faang project\\env\\lib\\site-packages (from mlflow-skinny==2.20.3->mlflow) (0.5.3)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.0.0 in z:\\faang project\\env\\lib\\site-packages (from mlflow-skinny==2.20.3->mlflow) (4.12.2)\n",
      "Requirement already satisfied: Mako in z:\\faang project\\env\\lib\\site-packages (from alembic!=1.10.0,<2->mlflow) (1.3.9)\n",
      "Requirement already satisfied: pywin32>=304 in z:\\faang project\\env\\lib\\site-packages (from docker<8,>=4.0.0->mlflow) (308)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in z:\\faang project\\env\\lib\\site-packages (from docker<8,>=4.0.0->mlflow) (2.3.0)\n",
      "Requirement already satisfied: Werkzeug>=3.1 in z:\\faang project\\env\\lib\\site-packages (from Flask<4->mlflow) (3.1.3)\n",
      "Requirement already satisfied: itsdangerous>=2.2 in z:\\faang project\\env\\lib\\site-packages (from Flask<4->mlflow) (2.2.0)\n",
      "Requirement already satisfied: blinker>=1.9 in z:\\faang project\\env\\lib\\site-packages (from Flask<4->mlflow) (1.9.0)\n",
      "Requirement already satisfied: graphql-core<3.3,>=3.1 in z:\\faang project\\env\\lib\\site-packages (from graphene<4->mlflow) (3.2.6)\n",
      "Requirement already satisfied: graphql-relay<3.3,>=3.1 in z:\\faang project\\env\\lib\\site-packages (from graphene<4->mlflow) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil<3,>=2.7.0 in z:\\faang project\\env\\lib\\site-packages (from graphene<4->mlflow) (2.9.0.post0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in z:\\faang project\\env\\lib\\site-packages (from Jinja2<4,>=3.0->mlflow) (3.0.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in z:\\faang project\\env\\lib\\site-packages (from matplotlib<4->mlflow) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in z:\\faang project\\env\\lib\\site-packages (from matplotlib<4->mlflow) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in z:\\faang project\\env\\lib\\site-packages (from matplotlib<4->mlflow) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in z:\\faang project\\env\\lib\\site-packages (from matplotlib<4->mlflow) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in z:\\faang project\\env\\lib\\site-packages (from matplotlib<4->mlflow) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in z:\\faang project\\env\\lib\\site-packages (from matplotlib<4->mlflow) (3.2.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in z:\\faang project\\env\\lib\\site-packages (from pandas<3->mlflow) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in z:\\faang project\\env\\lib\\site-packages (from pandas<3->mlflow) (2025.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in z:\\faang project\\env\\lib\\site-packages (from scikit-learn<2->mlflow) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in z:\\faang project\\env\\lib\\site-packages (from scikit-learn<2->mlflow) (3.5.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in z:\\faang project\\env\\lib\\site-packages (from sqlalchemy<3,>=1.4.0->mlflow) (3.1.1)\n",
      "Requirement already satisfied: colorama in z:\\faang project\\env\\lib\\site-packages (from click<9,>=7.0->mlflow-skinny==2.20.3->mlflow) (0.4.6)\n",
      "Requirement already satisfied: google-auth~=2.0 in z:\\faang project\\env\\lib\\site-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==2.20.3->mlflow) (2.38.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in z:\\faang project\\env\\lib\\site-packages (from gitpython<4,>=3.1.9->mlflow-skinny==2.20.3->mlflow) (4.0.12)\n",
      "Requirement already satisfied: zipp>=3.20 in z:\\faang project\\env\\lib\\site-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==2.20.3->mlflow) (3.21.0)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in z:\\faang project\\env\\lib\\site-packages (from opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.20.3->mlflow) (1.2.18)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.51b0 in z:\\faang project\\env\\lib\\site-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==2.20.3->mlflow) (0.51b0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in z:\\faang project\\env\\lib\\site-packages (from pydantic<3,>=1.10.8->mlflow-skinny==2.20.3->mlflow) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in z:\\faang project\\env\\lib\\site-packages (from pydantic<3,>=1.10.8->mlflow-skinny==2.20.3->mlflow) (2.27.2)\n",
      "Requirement already satisfied: six>=1.5 in z:\\faang project\\env\\lib\\site-packages (from python-dateutil<3,>=2.7.0->graphene<4->mlflow) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in z:\\faang project\\env\\lib\\site-packages (from requests<3,>=2.17.3->mlflow-skinny==2.20.3->mlflow) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in z:\\faang project\\env\\lib\\site-packages (from requests<3,>=2.17.3->mlflow-skinny==2.20.3->mlflow) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in z:\\faang project\\env\\lib\\site-packages (from requests<3,>=2.17.3->mlflow-skinny==2.20.3->mlflow) (2025.1.31)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in z:\\faang project\\env\\lib\\site-packages (from deprecated>=1.2.6->opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.20.3->mlflow) (1.17.2)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in z:\\faang project\\env\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==2.20.3->mlflow) (5.0.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in z:\\faang project\\env\\lib\\site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.20.3->mlflow) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in z:\\faang project\\env\\lib\\site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.20.3->mlflow) (4.9)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in z:\\faang project\\env\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.20.3->mlflow) (0.6.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock=pd.read_csv(r\"Z:\\FAANG project\\FAANG_DATA_MLFLOW.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "features = [\n",
    "    'Open', 'High', 'Low', 'Volume',\n",
    "    'Company_Amazon', 'Company_Apple', 'Company_Facebook', 'Company_Google', 'Company_Netflix'\n",
    "]\n",
    "target = 'Close'\n",
    "\n",
    "X = stock[features]\n",
    "y = stock[target]\n",
    "X_log_transformed = X.apply(np.log1p)  # Features\n",
    "y_log_transformed = np.log1p(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_log_transformed, y_log_transformed, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 23055 entries, 0 to 23054\n",
      "Data columns (total 18 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   Company           23055 non-null  object \n",
      " 1   Ticker            23055 non-null  object \n",
      " 2   Date              23055 non-null  object \n",
      " 3   Open              23055 non-null  float64\n",
      " 4   High              23055 non-null  float64\n",
      " 5   Low               23055 non-null  float64\n",
      " 6   Close             23055 non-null  float64\n",
      " 7   Adj Close         23055 non-null  float64\n",
      " 8   Volume            23055 non-null  int64  \n",
      " 9   Year              23055 non-null  int64  \n",
      " 10  Month             23055 non-null  int64  \n",
      " 11  Day               23055 non-null  int64  \n",
      " 12  Day_of_Week       23055 non-null  int64  \n",
      " 13  Company_Amazon    23055 non-null  float64\n",
      " 14  Company_Apple     23055 non-null  float64\n",
      " 15  Company_Facebook  23055 non-null  float64\n",
      " 16  Company_Google    23055 non-null  float64\n",
      " 17  Company_Netflix   23055 non-null  float64\n",
      "dtypes: float64(10), int64(5), object(3)\n",
      "memory usage: 3.2+ MB\n"
     ]
    }
   ],
   "source": [
    "stock.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/03/04 12:54:40 INFO mlflow.tracking.fluent: Experiment with name 'LinearRegression5' does not exist. Creating a new experiment.\n",
      "2025/03/04 12:54:44 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance:\n",
      " MAE: 27.0462\n",
      " RMSE: 34.7045\n",
      " R2 Score: 0.8598\n",
      " Mean CV R2: 0.8317\n",
      "üèÉ View run Linear_Regression_Model at: http://localhost:5000/#/experiments/106023816214055466/runs/4d646fb5a247418c9fae22df82c92e27\n",
      "üß™ View experiment at: http://localhost:5000/#/experiments/106023816214055466\n"
     ]
    }
   ],
   "source": [
    "#1. Linear_Regression\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.linear_model import Ridge  # Using Ridge instead of Lasso\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "# Set MLflow Tracking URI\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "\n",
    "# # Set Experiment\n",
    "mlflow.set_experiment(\"LinearRegression5\")\n",
    "\n",
    "# Generate synthetic dataset with more noise to prevent overfitting\n",
    "X, y = make_regression(n_samples=200, n_features=1, noise=30, random_state=42)\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Feature Scaling (MinMaxScaler)\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Ensure no active MLflow runs\n",
    "if mlflow.active_run():\n",
    "    mlflow.end_run()\n",
    "\n",
    "try:\n",
    "    with mlflow.start_run(run_name=\"Linear_Regression_Model\"):\n",
    "        # Set Regularization Parameter (Increased alpha)\n",
    "        alpha = 1.0  \n",
    "\n",
    "        # Log Parameters\n",
    "        mlflow.log_param(\"model\", \"Linear__Regression\")\n",
    "        mlflow.log_param(\"alpha\", alpha)\n",
    "        mlflow.log_param(\"test_size\", 0.2)\n",
    "        mlflow.log_param(\"random_state\", 42)\n",
    "\n",
    "        # Model Training (Using Ridge instead of Lasso)\n",
    "        model = Ridge(alpha=alpha)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Predictions\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        # Cross-Validation Score\n",
    "        cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring=\"r2\")\n",
    "        mean_cv_r2 = np.mean(cv_scores)\n",
    "\n",
    "        # Evaluation Metrics\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "        # Log Metrics\n",
    "        mlflow.log_metric(\"MAE\", mae)\n",
    "        mlflow.log_metric(\"RMSE\", rmse)\n",
    "        mlflow.log_metric(\"R2 Score\", r2)\n",
    "        mlflow.log_metric(\"Mean CV R2\", mean_cv_r2)\n",
    "\n",
    "        # Log Model\n",
    "        mlflow.sklearn.log_model(model, \"Linear__Regression_model\")\n",
    "\n",
    "        # Print Performance\n",
    "        print(f\"Model Performance:\\n MAE: {mae:.4f}\\n RMSE: {rmse:.4f}\\n R2 Score: {r2:.4f}\\n Mean CV R2: {mean_cv_r2:.4f}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    mlflow.end_run(status=\"FAILED\")\n",
    "\n",
    "finally:\n",
    "    # Ensure the run ends even if an error occurs\n",
    "    if mlflow.active_run():\n",
    "        mlflow.end_run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation R2 Scores: [0.85093091 0.83821302 0.82622612 0.74228458 0.83675616]\n",
      "Mean Cross-Validation R2 Score: 0.82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/03/04 12:54:49 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run DecisionTree_regression_model at: http://localhost:5000/#/experiments/106023816214055466/runs/11945d7eca5347dcac1ec883cdab26b6\n",
      "üß™ View experiment at: http://localhost:5000/#/experiments/106023816214055466\n"
     ]
    }
   ],
   "source": [
    "#2. DecisionTree_Regression\n",
    "#importing necessery libraries\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "import mlflow\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Ensure no active runs\n",
    "if mlflow.active_run():\n",
    "    mlflow.end_run()\n",
    "\n",
    "try:\n",
    "    with mlflow.start_run(run_name=\"DecisionTree_regression_model\"):\n",
    "        # Log Parameters\n",
    "        mlflow.log_param(\"model\", \"Decision Tree Regression\")\n",
    "        mlflow.log_param(\"test_size\", 0.2)\n",
    "        mlflow.log_param(\"random_state\", 42)\n",
    "        mlflow.log_param(\"max_depth\", 5)\n",
    "        mlflow.log_param(\"min_samples_split\", 10)\n",
    "        mlflow.log_param(\"min_samples_leaf\", 5)\n",
    "        mlflow.log_param(\"ccp_alpha\", 0.01)\n",
    "\n",
    "        # Model Training with regularization\n",
    "        model = DecisionTreeRegressor(\n",
    "            max_depth=5,\n",
    "            min_samples_split=10,\n",
    "            min_samples_leaf=5,\n",
    "            ccp_alpha=0.01,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        scores = cross_val_score(model, X_train, y_train, cv=kf, scoring='r2')\n",
    "        mean_cv_r2 = np.mean(scores)\n",
    "        \n",
    "        print(f'Cross-Validation R2 Scores: {scores}')\n",
    "        print(f'Mean Cross-Validation R2 Score: {mean_cv_r2:.2f}')\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Predictions\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Evaluation Metrics\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        \n",
    "        # Log Metrics\n",
    "        mlflow.log_metric(\"MAE\", mae)\n",
    "        mlflow.log_metric(\"RMSE\", rmse)\n",
    "        mlflow.log_metric(\"R2 Score\", r2)\n",
    "        mlflow.log_metric(\"Mean CV R2\", mean_cv_r2)\n",
    "\n",
    "        # Log Model\n",
    "        mlflow.sklearn.log_model(model, \"DecisionTree_regression_model\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    mlflow.end_run(status=\"FAILED\")\n",
    "\n",
    "finally:\n",
    "    # Ensure the run ends even if an error occurs\n",
    "    if mlflow.active_run():\n",
    "        mlflow.end_run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "\n",
      "üîπ Best Parameters: {'max_depth': 5, 'min_samples_leaf': 4, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "\n",
      "üìä Cross-Validation R2 Scores: [0.81589529 0.81912663 0.81792934 0.82145491 0.81927456 0.82214507\n",
      " 0.82137402 0.8235414  0.83513238 0.83593198 0.83513238 0.83593198\n",
      " 0.78579952 0.78951729 0.80406093 0.80707175 0.80628826 0.80903805\n",
      " 0.81164387 0.81308607 0.83417101 0.83473456 0.83417101 0.83473456\n",
      " 0.78446011 0.78802634 0.80353145 0.80659925 0.80627528 0.80904398\n",
      " 0.81162493 0.81309311 0.83417101 0.83473456 0.83417101 0.83473456]\n",
      "üìà Mean CV R2: 0.8174\n",
      "\n",
      "üìä Model Performance:\n",
      "üî∏ MAE: 29.5252\n",
      "üîπ RMSE: 35.6956\n",
      "üìà R2 Score: 0.8517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/03/04 12:55:00 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run RandomForest_regression_model at: http://localhost:5000/#/experiments/106023816214055466/runs/9c37d7167fca43cfadc1c30f9d960dd9\n",
      "üß™ View experiment at: http://localhost:5000/#/experiments/106023816214055466\n"
     ]
    }
   ],
   "source": [
    "#3. RandomForest_Regression\n",
    "\n",
    "#importing necessery libraries\n",
    "import mlflow\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Ensure no active runs\n",
    "if mlflow.active_run():\n",
    "    mlflow.end_run()\n",
    "\n",
    "try:\n",
    "    with mlflow.start_run(run_name=\"RandomForest_regression_model\"):\n",
    "        # Log Parameters\n",
    "        mlflow.log_param(\"model\", \"Random Forest Regression\")\n",
    "        mlflow.log_param(\"test_size\", 0.2)\n",
    "        mlflow.log_param(\"random_state\", 42)\n",
    "\n",
    "        # Define Model & Parameter Grid\n",
    "        model = RandomForestRegressor(random_state=42)\n",
    "        param_grid = {\n",
    "            'n_estimators': [50, 100],\n",
    "            'max_depth': [5, 10, None],\n",
    "            'min_samples_split': [2, 5],\n",
    "            'min_samples_leaf': [1, 2, 4]\n",
    "        }\n",
    "\n",
    "        # Perform Grid Search with 5-Fold Cross-Validation\n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=model, \n",
    "            param_grid=param_grid, \n",
    "            cv=5, \n",
    "            scoring='r2', \n",
    "            n_jobs=-1, \n",
    "            verbose=2\n",
    "        )\n",
    "        grid_search.fit(X_train, y_train)\n",
    "\n",
    "        # Best parameters from GridSearch\n",
    "        best_model = grid_search.best_estimator_\n",
    "        best_params = grid_search.best_params_\n",
    "        print(f\"\\nüîπ Best Parameters: {best_params}\")\n",
    "        mlflow.log_params(best_params)\n",
    "\n",
    "        # Print Cross-Validation Results\n",
    "        cv_results = grid_search.cv_results_\n",
    "        mean_test_scores = cv_results[\"mean_test_score\"]\n",
    "        print(\"\\nüìä Cross-Validation R2 Scores:\", mean_test_scores)\n",
    "        print(f\"üìà Mean CV R2: {np.mean(mean_test_scores):.4f}\")\n",
    "        mlflow.log_metric(\"Mean CV R2\", np.mean(mean_test_scores))\n",
    "\n",
    "        # Predictions\n",
    "        y_pred = best_model.predict(X_test)\n",
    "\n",
    "        # Evaluation Metrics\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "        # Log Metrics\n",
    "        mlflow.log_metric(\"MAE\", mae)\n",
    "        mlflow.log_metric(\"RMSE\", rmse)\n",
    "        mlflow.log_metric(\"R2 Score\", r2)\n",
    "\n",
    "        # Display Scores\n",
    "        print(\"\\nüìä Model Performance:\")\n",
    "        print(f\"üî∏ MAE: {mae:.4f}\")\n",
    "        print(f\"üîπ RMSE: {rmse:.4f}\")\n",
    "        print(f\"üìà R2 Score: {r2:.4f}\")\n",
    "\n",
    "        # Log Model to MLflow\n",
    "        mlflow.sklearn.log_model(best_model, \"RandomForest_regression_model\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    mlflow.end_run(status=\"FAILED\")\n",
    "\n",
    "finally:\n",
    "    # Ensure the run ends even if an error occurs\n",
    "    if mlflow.active_run():\n",
    "        mlflow.end_run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in z:\\faang project\\env\\lib\\site-packages (2.1.4)\n",
      "Requirement already satisfied: numpy in z:\\faang project\\env\\lib\\site-packages (from xgboost) (2.2.3)\n",
      "Requirement already satisfied: scipy in z:\\faang project\\env\\lib\\site-packages (from xgboost) (1.15.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training XGBoost Regression Model\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "Best Parameters: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 20}\n",
      "Cross-Validation R2 Scores: [0.82141301 0.84798175 0.82680591 0.72997133 0.89099156]\n",
      "Mean CV R2: 0.8234\n",
      "\n",
      "üìä Model Performance:\n",
      "MAE: 32.0083\n",
      "RMSE: 40.1013\n",
      "R2 Score: 0.8128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/03/04 12:55:10 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run XGBoost_regression_model at: http://localhost:5000/#/experiments/106023816214055466/runs/94dfbe6e13244f32a14cd7816326e840\n",
      "üß™ View experiment at: http://localhost:5000/#/experiments/106023816214055466\n"
     ]
    }
   ],
   "source": [
    "#4. XGBoost_regression_model\n",
    "\n",
    "#importing necessary libraries \n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.model_selection import KFold, cross_val_score, GridSearchCV\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Ensure no active runs\n",
    "if mlflow.active_run():\n",
    "    mlflow.end_run()\n",
    "\n",
    "try:\n",
    "    with mlflow.start_run(run_name=\"XGBoost_regression_model\"):\n",
    "        print(\"Training XGBoost Regression Model\")\n",
    "\n",
    "        # Log Parameters\n",
    "        mlflow.log_param(\"model\", \"XGBoost Regression\")\n",
    "        mlflow.log_param(\"test_size\", 0.2)\n",
    "        mlflow.log_param(\"random_state\", 42)\n",
    "\n",
    "        # Initialize Model\n",
    "        model = XGBRegressor(random_state=42)\n",
    "\n",
    "        param_grid = {\n",
    "            'n_estimators': [10, 20],\n",
    "            'max_depth': [3, 5],\n",
    "            'learning_rate': [0.01, 0.1]\n",
    "        }\n",
    "\n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=model,\n",
    "            param_grid=param_grid,\n",
    "            cv=5,\n",
    "            scoring='r2',\n",
    "            n_jobs=-1,\n",
    "            verbose=2\n",
    "        )\n",
    "\n",
    "        # Fit Model with GridSearch\n",
    "        grid_search.fit(X_train, y_train)\n",
    "\n",
    "        # Best Parameters\n",
    "        best_model = grid_search.best_estimator_\n",
    "        print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "        mlflow.log_params(grid_search.best_params_)\n",
    "\n",
    "        # Cross-validation R2 Scores\n",
    "        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        cv_scores = cross_val_score(best_model, X_train, y_train, cv=kf, scoring='r2')\n",
    "\n",
    "        print(f\"Cross-Validation R2 Scores: {cv_scores}\")\n",
    "        print(f\"Mean CV R2: {np.mean(cv_scores):.4f}\")\n",
    "        mlflow.log_metric(\"Mean_CV_R2\", np.mean(cv_scores))\n",
    "\n",
    "        # Predictions\n",
    "        y_pred = best_model.predict(X_test)\n",
    "\n",
    "        # Evaluation Metrics\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "        print(\"\\nüìä Model Performance:\")\n",
    "        print(f\"MAE: {mae:.4f}\")\n",
    "        print(f\"RMSE: {rmse:.4f}\")\n",
    "        print(f\"R2 Score: {r2:.4f}\")\n",
    "\n",
    "        # Log Metrics\n",
    "        mlflow.log_metric(\"MAE\", mae)\n",
    "        mlflow.log_metric(\"RMSE\", rmse)\n",
    "        mlflow.log_metric(\"R2 Score\", r2)\n",
    "\n",
    "        # Log Model\n",
    "        mlflow.sklearn.log_model(\n",
    "            sk_model=best_model,\n",
    "            artifact_path=\"XGBoost_regression_model\"\n",
    "        )\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    mlflow.end_run(status=\"FAILED\")\n",
    "\n",
    "finally:\n",
    "    # Ensure the run ends even if an error occurs\n",
    "    if mlflow.active_run():\n",
    "        mlflow.end_run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LightGBM Regression Model\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000177 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 54\n",
      "[LightGBM] [Info] Number of data points in the train set: 160, number of used features: 1\n",
      "[LightGBM] [Info] Start training from score 2.920178\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Best Parameters: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 20}\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000184 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 44\n",
      "[LightGBM] [Info] Number of data points in the train set: 128, number of used features: 1\n",
      "[LightGBM] [Info] Start training from score 1.456819\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000068 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 44\n",
      "[LightGBM] [Info] Number of data points in the train set: 128, number of used features: 1\n",
      "[LightGBM] [Info] Start training from score 4.527189\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000021 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 44\n",
      "[LightGBM] [Info] Number of data points in the train set: 128, number of used features: 1\n",
      "[LightGBM] [Info] Start training from score 2.287952\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000078 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 44\n",
      "[LightGBM] [Info] Number of data points in the train set: 128, number of used features: 1\n",
      "[LightGBM] [Info] Start training from score 2.002929\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000024 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 44\n",
      "[LightGBM] [Info] Number of data points in the train set: 128, number of used features: 1\n",
      "[LightGBM] [Info] Start training from score 4.326003\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Cross-Validation R2 Scores: [0.81090714 0.79545688 0.83475347 0.69211473 0.84396305]\n",
      "Mean CV R2: 0.7954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "z:\\FAANG project\\env\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "z:\\FAANG project\\env\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "z:\\FAANG project\\env\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "z:\\FAANG project\\env\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "z:\\FAANG project\\env\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "z:\\FAANG project\\env\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "2025/03/04 12:55:17 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run LightGBM_Regression_Model at: http://localhost:5000/#/experiments/106023816214055466/runs/63952bf9874a464d91102abca07a2697\n",
      "üß™ View experiment at: http://localhost:5000/#/experiments/106023816214055466\n"
     ]
    }
   ],
   "source": [
    "#5.LightGBM_Regression_Model\n",
    "\n",
    "#importing necessery libraries\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV, KFold, cross_val_score\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Ensure no active runs\n",
    "if mlflow.active_run():\n",
    "    mlflow.end_run()\n",
    "\n",
    "try:\n",
    "    with mlflow.start_run(run_name=\"LightGBM_Regression_Model\"):\n",
    "        print(\"Training LightGBM Regression Model\")\n",
    "\n",
    "        # Log Parameters\n",
    "        mlflow.log_param(\"model\", \"LightGBM Regression\")\n",
    "        mlflow.log_param(\"test_size\", 0.2)\n",
    "        mlflow.log_param(\"random_state\", 42)\n",
    "\n",
    "        # Initialize Model and GridSearchCV\n",
    "        model = LGBMRegressor(random_state=42)\n",
    "        param_grid = {\n",
    "            'n_estimators': [10, 20],\n",
    "            'max_depth': [-1, 10],\n",
    "            'learning_rate': [0.01, 0.1]\n",
    "        }\n",
    "\n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=model,\n",
    "            param_grid=param_grid,\n",
    "            cv=5,\n",
    "            scoring='r2',\n",
    "            n_jobs=-1,\n",
    "            verbose=2\n",
    "        )\n",
    "\n",
    "        # Fit Model with GridSearch\n",
    "        grid_search.fit(X_train, y_train)\n",
    "\n",
    "        # Best Parameters\n",
    "        best_model = grid_search.best_estimator_\n",
    "        print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "        mlflow.log_params(grid_search.best_params_)\n",
    "\n",
    "        # Cross-Validation R¬≤ Score\n",
    "        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        cv_scores = cross_val_score(best_model, X_train, y_train, cv=kf, scoring='r2')\n",
    "\n",
    "        print(f\"Cross-Validation R2 Scores: {cv_scores}\")\n",
    "        print(f\"Mean CV R2: {np.mean(cv_scores):.4f}\")\n",
    "        mlflow.log_metric(\"Mean_CV_R2\", np.mean(cv_scores))\n",
    "\n",
    "        # Predictions\n",
    "        y_pred = best_model.predict(X_test)\n",
    "\n",
    "        # Evaluation Metrics\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "        # Log Metrics\n",
    "        mlflow.log_metric(\"MAE\", mae)\n",
    "        mlflow.log_metric(\"RMSE\", rmse)\n",
    "        mlflow.log_metric(\"R2 Score\", r2)\n",
    "\n",
    "        # Log Model with Signature\n",
    "        mlflow.sklearn.log_model(\n",
    "            sk_model=best_model,\n",
    "            artifact_path=\"LightGBM_Regression_Model\"\n",
    "        )\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    mlflow.end_run(status=\"FAILED\")\n",
    "\n",
    "finally:\n",
    "    # Ensure the run ends even if an error occurs\n",
    "    if mlflow.active_run():\n",
    "        mlflow.end_run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training XGBoost Serial Regression Model\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "Best Parameters: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 20}\n",
      "Cross-Validation R2 Scores: [0.82141301 0.84798175 0.82680591 0.72997133 0.89099156]\n",
      "Mean CV R2: 0.8234\n",
      "\n",
      "üìä Model Performance:\n",
      "MAE: 32.0083\n",
      "RMSE: 40.1013\n",
      "R2 Score: 0.8128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/03/04 12:55:22 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run XGBoost_serial_regression_model at: http://localhost:5000/#/experiments/106023816214055466/runs/34b531940a4b438c9266108079da2029\n",
      "üß™ View experiment at: http://localhost:5000/#/experiments/106023816214055466\n"
     ]
    }
   ],
   "source": [
    "#6.XGBoost_serial_regression_model \n",
    "\n",
    "# Importing necessary libraries\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.model_selection import KFold, cross_val_score, GridSearchCV\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Ensure no active runs\n",
    "if mlflow.active_run():\n",
    "    mlflow.end_run()\n",
    "\n",
    "try:\n",
    "    with mlflow.start_run(run_name=\"XGBoost_serial_regression_model\"):\n",
    "        print(\"Training XGBoost Serial Regression Model\")\n",
    "\n",
    "        # Log Parameters\n",
    "        mlflow.log_param(\"model\", \"XGBoost Serial Regression\")\n",
    "        mlflow.log_param(\"test_size\", 0.2)\n",
    "        mlflow.log_param(\"random_state\", 42)\n",
    "\n",
    "        # Initialize Model\n",
    "        model = XGBRegressor(random_state=42)\n",
    "\n",
    "        param_grid = {\n",
    "            'n_estimators': [10, 20],\n",
    "            'max_depth': [3, 5],\n",
    "            'learning_rate': [0.01, 0.1]\n",
    "        }\n",
    "\n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=model,\n",
    "            param_grid=param_grid,\n",
    "            cv=5,\n",
    "            scoring='r2',\n",
    "            n_jobs=-1,\n",
    "            verbose=2\n",
    "        )\n",
    "\n",
    "        # Fit Model with GridSearch\n",
    "        grid_search.fit(X_train, y_train)\n",
    "\n",
    "        # Best Parameters\n",
    "        best_model = grid_search.best_estimator_\n",
    "        print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "        mlflow.log_params(grid_search.best_params_)\n",
    "\n",
    "        # Cross-validation R2 Scores\n",
    "        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        cv_scores = cross_val_score(best_model, X_train, y_train, cv=kf, scoring='r2')\n",
    "\n",
    "        print(f\"Cross-Validation R2 Scores: {cv_scores}\")\n",
    "        print(f\"Mean CV R2: {np.mean(cv_scores):.4f}\")\n",
    "        mlflow.log_metric(\"Mean_CV_R2\", np.mean(cv_scores))\n",
    "\n",
    "        # Predictions\n",
    "        y_pred = best_model.predict(X_test)\n",
    "\n",
    "        # Evaluation Metrics\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "        print(\"\\nüìä Model Performance:\")\n",
    "        print(f\"MAE: {mae:.4f}\")\n",
    "        print(f\"RMSE: {rmse:.4f}\")\n",
    "        print(f\"R2 Score: {r2:.4f}\")\n",
    "\n",
    "        # Log Metrics\n",
    "        mlflow.log_metric(\"MAE\", mae)\n",
    "        mlflow.log_metric(\"RMSE\", rmse)\n",
    "        mlflow.log_metric(\"R2 Score\", r2)\n",
    "\n",
    "        # Log Model\n",
    "        mlflow.sklearn.log_model(\n",
    "            sk_model=best_model,\n",
    "            artifact_path=\"XGBoost_serial_regression_model\"\n",
    "        )\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    mlflow.end_run(status=\"FAILED\")\n",
    "\n",
    "finally:\n",
    "    # Ensure the run ends even if an error occurs\n",
    "    if mlflow.active_run():\n",
    "        mlflow.end_run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['LinearRegression']\n"
     ]
    }
   ],
   "source": [
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "client = MlflowClient()\n",
    "\n",
    "registered_models = client.search_registered_models()\n",
    "print([model.name for model in registered_models])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
